defaults:
  - _self_
  - experiment: variants

train: # Not used in generate.py
  name: null # Name of experiment (prefix of experiment name)
             # This will also be used as the W&B run name
  ckpt_epoch: max
  epochs_per_ckpt: 5
  iters_per_logging: 10
  n_epochs: 10
  learning_rate: 2e-4
  batch_size_per_gpu: 8

generate:
  name: null # Only relevant when using generate.py
  ckpt_epoch: max # Which checkpoint to use; assign a number or "max". Is ignored when sampling during training
  n_samples: 8 # Number of utterances to be generated (per GPU)
  batch_size: null # Number of samples to generate at once per GPU. null means max (equal to samples_per_gpu)
  # Path to conditional input on disk in brain-conditional setting, or word class in class-conditional setting. 
  # Can be null in unconditional setting.
  conditional_signal: /home/passch/data/HP1_ECoG_conditional/denoised_fixed-transcript/dag7.npy 
  # Can be "brain" for ECoG input or "class" for class-conditional sampling. Ignored if conditional_signal=null
  conditional_type: brain

distributed:
  dist_backend: nccl
  dist_url: tcp://localhost:54321

wandb:
  mode: disabled # Pass in 'wandb.mode=online' to turn on wandb logging
  project: brain2speech-diffusion
  entity: pascalschroeder
  id: null # Set to string and pass '+wandb.resume=true' to resume logging from run. 
           # ID of previous run be found in the run's URL on W&B (not the run name)
  job_type: training
