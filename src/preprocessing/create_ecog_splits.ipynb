{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splits for Brain2Speech Models\n",
    "\n",
    "Use the code in this notebook to create training and validation splits for the training of the brain2speech models. We first create splits for the ECoG data created using `src/preprocessing/create_ecog_data.py`, and then adjust the splits for the VariaNTS data created using `src/preprocessing/create_speech_data.py` to accomodate the skewed distribution of the ECoG words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils.generic import get_word_from_filepath"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain Data\n",
    "\n",
    "First get the names of the sound files from the paired ECoG+voice recording dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bed1.wav', 'bed2.wav', 'boel1.wav', 'brief1.wav', 'brief2.wav', 'brief3.wav', 'brief4.wav', 'brief5.wav', 'bril1.wav', 'bril2.wav', 'dag1.wav', 'dag10.wav', 'dag11.wav', 'dag12.wav', 'dag13.wav', 'dag2.wav', 'dag3.wav', 'dag4.wav', 'dag5.wav', 'dag6.wav', 'dag7.wav', 'dag8.wav', 'dag9.wav', 'dier1.wav', 'doel1.wav', 'dood1.wav', 'dood2.wav', 'dood3.wav', 'feest1.wav', 'goed1.wav', 'goed2.wav', 'goed3.wav', 'goed4.wav', 'goed5.wav', 'goed6.wav', 'goed7.wav', 'goed8.wav', 'goed9.wav', 'greep1.wav', 'greep2.wav', 'half1.wav', 'half2.wav', 'half3.wav', 'half4.wav', 'half5.wav', 'hand1.wav', 'hand2.wav', 'hand3.wav', 'heel1.wav', 'heel2.wav', 'heel3.wav', 'heel4.wav', 'heel5.wav', 'heel6.wav', 'heel7.wav', 'heer1.wav', 'hoofd1.wav', 'hoofd2.wav', 'hoofd3.wav', 'hoofd4.wav', 'hoofd5.wav', 'hoofd6.wav', 'hoofd7.wav', 'hoofd8.wav', 'hoop1.wav', 'hoop2.wav', 'hoop3.wav', 'hoop4.wav', 'kalm1.wav', 'kan1.wav', 'kan2.wav', 'kan3.wav', 'kan4.wav', 'kan5.wav', 'kan6.wav', 'kan7.wav', 'kant1.wav', 'kant2.wav', 'keel1.wav', 'keer1.wav', 'keer2.wav', 'keer3.wav', 'keer4.wav', 'keer5.wav', 'keer6.wav', 'keer7.wav', 'kind1.wav', 'kind2.wav', 'klein1.wav', 'klein2.wav', 'klein3.wav', 'kneep1.wav', 'kwaad1.wav', 'laat1.wav', 'laat2.wav', 'land1.wav', 'land2.wav', 'land3.wav', 'land4.wav', 'land5.wav', 'lang1.wav', 'lang2.wav', 'lang3.wav', 'lang4.wav', 'lang5.wav', 'lang6.wav', 'lang7.wav', 'lang8.wav', 'licht1.wav', 'los1.wav', 'los2.wav', 'los3.wav', 'los4.wav', 'los5.wav', 'los6.wav', 'man1.wav', 'man2.wav', 'man3.wav', 'man4.wav', 'meer1.wav', 'meer2.wav', 'meer3.wav', 'meer4.wav', 'meer5.wav', 'meer6.wav', 'meer7.wav', 'mond1.wav', 'mond2.wav', 'neus1.wav', 'neus2.wav', 'paar1.wav', 'paar2.wav', 'paar3.wav', 'paar4.wav', 'paar5.wav', 'paar6.wav', 'paar7.wav', 'paar8.wav', 'raam1.wav', 'raam2.wav', 'raam3.wav', 'rest1.wav', 'snel1.wav', 'stad1.wav', 'stad2.wav', 'stijf1.wav', 'stuur1.wav', 'tijd1.wav', 'tijd2.wav', 'tijd3.wav', 'tijd4.wav', 'vol1.wav', 'vorm1.wav', 'vorm2.wav', 'vroeg1.wav', 'vroeg2.wav', 'vroeg3.wav', 'vroeg4.wav', 'vroeg5.wav', 'vroeg6.wav', 'weg1.wav', 'weg2.wav', 'weg3.wav', 'weg4.wav', 'wel1.wav', 'wel10.wav', 'wel11.wav', 'wel12.wav', 'wel13.wav', 'wel14.wav', 'wel15.wav', 'wel16.wav', 'wel17.wav', 'wel18.wav', 'wel2.wav', 'wel3.wav', 'wel4.wav', 'wel5.wav', 'wel6.wav', 'wel7.wav', 'wel8.wav', 'wel9.wav', 'werk1.wav', 'werk2.wav', 'wind1.wav', 'zet1.wav', 'zin1.wav', 'zoon1.wav']\n"
     ]
    }
   ],
   "source": [
    "hp_data = '../../data/HP1_ECoG_conditional/sub-002'\n",
    "\n",
    "files = [file for file in os.listdir(hp_data) if file.endswith('.wav')]\n",
    "\n",
    "print(sorted(files))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the word counts for this data (words that occur less than 4 times are not shown) for Subject 2:\n",
    "\n",
    "WORD | wel | dag | goed | paar | hoofd | lang | heel | kan | meer | keer | vroeg | los | brief | half | land | hoop | weg | man | tijd\n",
    "---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---\n",
    "COUNT | 18 | 13 | 9 | 8 | 8 | 8 | 7 | 7 | 7 | 7 | 6 | 6 | 5Â | 5 | 5 | 4 | 4 | 4 | 4\n",
    "\n",
    "\n",
    "\n",
    "Because the data is so unevenly distributed, we take different amounts for the validation set for each word. Below we define the amounts per word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_choose = {'wel': 5, 'dag': 3, 'goed': 2, 'paar': 1, 'hoofd': 1, 'lang': 1, 'heel': 1, 'kan': 1, 'meer': 1, 'keer': 1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data for subject 1 are differently distributed, and that we may choose different amounts per word there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we randomly choose the designated number for each word from the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wel12.wav', 'wel17.wav', 'wel5.wav', 'wel7.wav', 'wel1.wav', 'dag2.wav', 'dag4.wav', 'dag9.wav', 'goed8.wav', 'goed4.wav', 'paar1.wav', 'hoofd7.wav', 'lang8.wav', 'heel6.wav', 'kan4.wav', 'meer6.wav', 'keer1.wav']\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(1144)\n",
    "\n",
    "val_files = []\n",
    "\n",
    "for (word, count) in words_to_choose.items():\n",
    "    word_files = [file for file in files if get_word_from_filepath(file) == word]\n",
    "    val_files.extend(\n",
    "        rng.choice(word_files, size=count, replace=False)\n",
    "    )\n",
    "\n",
    "print(val_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write train and validation splits from these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_path = '../../data/datasplits/HP1_ECoG_conditional/sub-002/'\n",
    "\n",
    "os.makedirs(splits_path, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [file for file in files if not file in val_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(splits_path + 'train.csv', 'w') as f:\n",
    "    f.write(','.join(train_files))\n",
    "with open(splits_path + 'val.csv', 'w') as f:\n",
    "    f.write(','.join(val_files))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VariaNTS Data\n",
    "\n",
    "For the brain conditional finetuning experiment that maps ECoG to VariaNTS data, we also need to change the train-val-split for the VariaNTS data, such that it fits to the just created split. Of course, we have to reuse the same splits as used in pretraining.\n",
    "\n",
    "The VariaNTS datasplits were created such that the validation set contains 3 randomly chosen speakers for each word. Since the ECoG validation set does not contain every word because of the uneven distribution of words, we can only keep the words in the VariaNTS validation split that are also in the ECoG split."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the original VariaNTS datasplits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27115, 165)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The splits used in both unconditional and class-conditional pretraining runs\n",
    "vnts_splits_path = '../../data/datasplits/VariaNTS/HP_90-10/'\n",
    "\n",
    "with open(vnts_splits_path + 'train.csv', 'r') as f:\n",
    "    vnts_train_files = f.read().split(',')\n",
    "with open(vnts_splits_path + 'val.csv', 'r') as f:\n",
    "    vnts_val_files = f.read().split(',')\n",
    "\n",
    "len(vnts_train_files), len(vnts_val_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for both train and validation files, we get the unique words used in the ECoG data. \n",
    "\n",
    "We have to do this not only for validation, but also for training, because the ECoG data may not contain all 55 words (indeed for subject 2, it contains 53 words, while for subject 1 only 43)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_train_words = np.unique(\n",
    "    [get_word_from_filepath(file) for file in train_files])\n",
    "hp_val_words = np.unique(\n",
    "    [get_word_from_filepath(file) for file in val_files])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we remove all words from the VariaNTS training and validation splits that are not in the ECoG ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26129, 30)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vnts_train_files = [\n",
    "    file for file in vnts_train_files \n",
    "        if get_word_from_filepath(file) in hp_train_words\n",
    "]\n",
    "vnts_val_files = [\n",
    "    file for file in vnts_val_files \n",
    "        if get_word_from_filepath(file) in hp_val_words\n",
    "]\n",
    "\n",
    "len(vnts_train_files), len(vnts_val_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the amount of remaining files, we can see that more files were removed from the validation split than the training split, relatively speaking. This makes sense, as only 2 words have to be removed from training for subject 2, while the ECoG validation split only contains a much smaller selection of words to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vnts_new_splits_path = '../../data/datasplits/VariaNTS/HP_b2s_90-10/'\n",
    "\n",
    "os.makedirs(vnts_new_splits_path, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vnts_new_splits_path + 'train.csv', 'w') as f:\n",
    "    f.write(','.join(vnts_train_files))\n",
    "with open(vnts_new_splits_path + 'val.csv', 'w') as f:\n",
    "    f.write(','.join(vnts_val_files))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the unaugmented train files and augmented val files, for completeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1595 2805\n",
      "1537 510\n"
     ]
    }
   ],
   "source": [
    "with open(vnts_splits_path + 'train_noaug.csv', 'r') as f:\n",
    "    vnts_train_files = f.read().split(',')\n",
    "with open(vnts_splits_path + 'val_aug.csv', 'r') as f:\n",
    "    vnts_val_files = f.read().split(',')\n",
    "\n",
    "print(len(vnts_train_files), len(vnts_val_files))\n",
    "\n",
    "vnts_train_files = [\n",
    "    file for file in vnts_train_files \n",
    "        if get_word_from_filepath(file) in hp_train_words\n",
    "]\n",
    "vnts_val_files = [\n",
    "    file for file in vnts_val_files \n",
    "        if get_word_from_filepath(file) in hp_val_words\n",
    "]\n",
    "\n",
    "print(len(vnts_train_files), len(vnts_val_files))\n",
    "\n",
    "with open(vnts_new_splits_path + 'train_noaug.csv', 'w') as f:\n",
    "    f.write(','.join(vnts_train_files))\n",
    "with open(vnts_new_splits_path + 'val_aug.csv', 'w') as f:\n",
    "    f.write(','.join(vnts_val_files))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('thesis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d1ffe79cb1702ec543d226cdf22b974fcf7118d916c6f3b6ca5e225593afd5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
