{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Classifier Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, multilabel_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from models.brain_class_encoder import BrainClassifierV1, BrainClassifierV3\n",
    "from dataloaders.conditional_loaders import ClassConditionalLoader, ECOGLoader\n",
    "from utils.generic import get_word_from_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 17\n"
     ]
    }
   ],
   "source": [
    "data_base_dir = Path('/home/passch/data/')\n",
    "data_path = data_base_dir / 'HP1_ECoG_conditional/sub-002'\n",
    "splits_path = data_base_dir / 'datasplits/HP1_ECoG_conditional/sub-002'\n",
    "\n",
    "# Open train and val files from the precomputed splits. Since these were computed for the speech files, we replace the\n",
    "# .wav file endings with .npy to get the brain activity samples.\n",
    "with open(splits_path / 'train.csv', 'r') as f:\n",
    "    train_files = [fn.replace('.wav','.npy') for fn in f.read().split(',')]\n",
    "with open(splits_path / 'val.csv', 'r') as f:\n",
    "    val_files = [fn.replace('.wav','.npy') for fn in f.read().split(',')]\n",
    "\n",
    "# Set no. of classes to 55 even if dataset may have lower actual number of classes. \n",
    "# This ensures compatibility with the class-conditional pretraining model.\n",
    "n_classes = 55\n",
    "print(len(train_files), len(val_files))\n",
    "\n",
    "# Shuffle in-place\n",
    "rng = np.random.default_rng()\n",
    "rng.shuffle(train_files)\n",
    "rng.shuffle(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_loader = ClassConditionalLoader(data_base_dir / 'HP_VariaNTS_intersection.txt')\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, files) -> None:\n",
    "        super().__init__()\n",
    "        self.files=files\n",
    "    def __getitem__(self, n:int):\n",
    "        ecog = ECOGLoader.process_ecog(data_path / self.files[n])\n",
    "        word_vector = class_label_loader(self.files[n]).squeeze()\n",
    "        return ecog, word_vector\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def labels(self):\n",
    "        return [get_word_from_filepath(fp) for fp in self.files]\n",
    "\n",
    "trainset = Dataset(train_files)\n",
    "valset = Dataset(val_files)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=8, num_workers=4, pin_memory=False, shuffle=False, drop_last=False)\n",
    "\n",
    "# Make batch size == length of val files\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    valset, batch_size=len(val_files), num_workers=4, pin_memory=False, shuffle=False, drop_last=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-5\n",
    "N_EPOCHS = 1000\n",
    "\n",
    "SAVE_ITER = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BrainClassifierV1(\n",
       "  (network): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=2640, out_features=1320, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): LayerNorm((1320,), eps=1e-05, elementwise_affine=True)\n",
       "    (4): Dropout(p=0.4, inplace=False)\n",
       "    (5): Linear(in_features=1320, out_features=660, bias=True)\n",
       "    (6): ReLU()\n",
       "    (7): LayerNorm((660,), eps=1e-05, elementwise_affine=True)\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): Linear(in_features=660, out_features=55, bias=True)\n",
       "    (10): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BrainClassifierV1(in_nodes=48 * 55, n_classes=n_classes).cuda()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "start_epoch = 1\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp/Sub2_Full-Std_MLP_55classes_LR1e-05_dropout_layernorm_no-shuffle_3/\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = f'exp/Brain-Classifier-{datetime.now()}/'\n",
    "\n",
    "os.makedirs(ckpt_path, exist_ok=False)\n",
    "print(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {\n",
    "    'train_loss_epoch': [], 'train_loss_batch': [], 'train_acc_epoch': [], 'train_acc_batch': [],\n",
    "    'val_loss_epoch': [], 'val_loss_batch': [], 'val_acc_epoch': [], 'val_acc_batch': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1200/1200 [14:03<00:00,  1.42it/s, Train Loss=3.04, Val Loss=3.78, Train Acc=1.000, Val Acc=0.235]\n"
     ]
    }
   ],
   "source": [
    "for epoch in (pbar := tqdm(range(start_epoch, start_epoch+N_EPOCHS), desc='Training', ncols=125)):\n",
    "    # TRAINING\n",
    "    model.train()\n",
    "    train_loss_epoch = 0.\n",
    "    train_acc_epoch = 0.\n",
    "    for i, data in enumerate(trainloader):\n",
    "        x, y = data\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_batch = loss.item()\n",
    "        logs['train_loss_batch'].append(train_loss_batch)\n",
    "        train_loss_epoch += train_loss_batch\n",
    "\n",
    "        train_acc_batch = accuracy_score(\n",
    "            torch.argmax(y, dim=1).cpu(), torch.argmax(y_pred, dim=1).cpu())\n",
    "        logs['train_acc_batch'].append(train_acc_batch)\n",
    "        train_acc_epoch += train_acc_batch\n",
    "   \n",
    "    train_loss_epoch /= len(trainloader)\n",
    "    logs['train_loss_epoch'].append(train_loss_epoch)\n",
    "\n",
    "    train_acc_epoch /= len(trainloader)\n",
    "    logs['train_acc_epoch'].append(train_acc_epoch)\n",
    "\n",
    "    # VALIDATING\n",
    "    model.eval()\n",
    "    val_loss_epoch = 0.\n",
    "    val_acc_epoch = 0.\n",
    "    for i, data in enumerate(valloader):\n",
    "        x, y = data\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        val_loss_batch = loss.item()\n",
    "        logs['val_loss_batch'].append(val_loss_batch)\n",
    "        val_loss_epoch += val_loss_batch\n",
    "\n",
    "        val_acc_batch = accuracy_score(\n",
    "            torch.argmax(y, dim=1).cpu(), torch.argmax(y_pred, dim=1).cpu())\n",
    "        logs['val_acc_batch'].append(val_acc_batch)\n",
    "        val_acc_epoch += val_acc_batch\n",
    "\n",
    "    val_loss_epoch /= len(valloader)\n",
    "    logs['val_loss_epoch'].append(val_loss_epoch)\n",
    "\n",
    "    val_acc_epoch /= len(valloader)\n",
    "    logs['val_acc_epoch'].append(val_acc_epoch)\n",
    "\n",
    "    # Update loss values in progress bar\n",
    "    pbar.set_postfix({\n",
    "        'Train Loss': f'{logs[\"train_loss_epoch\"][-1]:.2f}', \n",
    "        'Val Loss': f'{logs[\"val_loss_epoch\"][-1]:.2f}',\n",
    "        'Train Acc': f'{logs[\"train_acc_epoch\"][-1]:.3f}',\n",
    "        'Val Acc': f'{logs[\"val_acc_epoch\"][-1]:.3f}',\n",
    "    })\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if epoch % SAVE_ITER == 0:\n",
    "        torch.save(\n",
    "            {'model_state_dict': model.state_dict()},\n",
    "            os.path.join(ckpt_path, f'{epoch}.pkl')\n",
    "        )\n",
    "\n",
    "start_epoch = epoch + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save logs to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ckpt_path, 'logs.json'), \"w\") as f:\n",
    "    json.dump(logs, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Logged Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 0\n",
    "end_idx = -1\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(logs['train_loss_epoch'][start_idx:end_idx], color='orange', label='Training')\n",
    "plt.plot(logs['val_loss_epoch'][start_idx:end_idx], color='blue', label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.legend(loc=3)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(logs['train_acc_epoch'][start_idx:end_idx], color='orange', label='Training')\n",
    "plt.plot(logs['val_acc_epoch'][start_idx:end_idx], color='blue', label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=4)\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(os.path.join(ckpt_path, 'loss_progression'))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_from_token(token):\n",
    "    return list(class_label_loader.word_tokens.keys())\\\n",
    "        [list(class_label_loader.word_tokens.values()).index(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_train = []\n",
    "all_true_train = []\n",
    "\n",
    "all_preds_val = []\n",
    "all_true_val = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i, data in enumerate(trainloader):\n",
    "    x, y = data\n",
    "    x = x.cuda()\n",
    "    \n",
    "    y_pred = model(x)\n",
    "\n",
    "    all_preds_train.extend(torch.argmax(y_pred, dim=1).cpu().numpy())\n",
    "    all_true_train.extend(torch.argmax(y, dim=1).numpy())\n",
    "\n",
    "for i, data in enumerate(valloader):\n",
    "    x, y = data\n",
    "    x = x.cuda()\n",
    "\n",
    "    y_pred = model(x)\n",
    "    all_preds_val.extend(torch.argmax(y_pred, dim=1).cpu().numpy())\n",
    "    all_true_val.extend(torch.argmax(y, dim=1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted class indices back to words\n",
    "all_preds_train = [get_word_from_token(l) for l in all_preds_train]\n",
    "all_true_train = [get_word_from_token(l) for l in all_true_train]\n",
    "all_preds_val = [get_word_from_token(l) for l in all_preds_val]\n",
    "all_true_val = [get_word_from_token(l) for l in all_true_val]\n",
    "\n",
    "print('Train acc.:', accuracy_score(all_true_train, all_preds_train))\n",
    "print('Val. acc. :', accuracy_score(all_true_val, all_preds_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export predictions\n",
    "save_dict = {\n",
    "    'train_preds': all_preds_train,\n",
    "    'train_true': all_true_train,\n",
    "    'val_preds': all_preds_val,\n",
    "    'val_true': all_true_val,\n",
    "}\n",
    "with open(os.path.join(ckpt_path, 'predictions_e{epoch}.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(save_dict, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'axes.labelsize': 18, \n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.set_title('Training', fontsize=24)\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    all_true_train, all_preds_train, ax=ax, colorbar=False, xticks_rotation=45)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ckpt_path, f'conf_mat_train_e{epoch}'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.set_title('Validation', fontsize=24)\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    all_true_val, all_preds_val, ax=ax, colorbar=False, xticks_rotation=45)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ckpt_path, f'conf_mat_val_e{epoch}'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffwave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a17d9c9cbbc962de36b255476254331786dd384e23a24706cf6aa287aa1626b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
