{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluations\n",
    "\n",
    "Quantitative evaluations of all speech generation models (SG-U, SG-C, B2S-Uv, B2S-Cv, B2S-Ur) using the Frechet Audio Distance and Inception Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "sys.path.append('..') # append code directory to path\n",
    "\n",
    "from frechet_audio_distance import FrechetAudioDistance, load_audio_task\n",
    "import numpy as np\n",
    "\n",
    "from utils.inception_score import InceptionScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/HP_VariaNTS_intersection.txt', 'r') as f:\n",
    "    words = f.read().split(',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inherit from the [`FrechetAudioDistance`](https://github.com/gudgud96/frechet-audio-distance) class to overwrite how it loads audio. By default, it loads all audio files in a given directory. However, we want to have more control over which files are loaded, e.g. to compare artificial and real speakers. Therefore, we change it to take a list of files instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFAD(FrechetAudioDistance):\n",
    "    def __init__(self, use_pca=False, use_activation=False, verbose=False, audio_load_worker=8):\n",
    "        super().__init__(use_pca, use_activation, verbose, audio_load_worker)\n",
    "\n",
    "    def load_audio_files_from_list(self, file_list):\n",
    "        with Pool(self.audio_load_worker) as p:\n",
    "            res_list = [\n",
    "                result for result in tqdm(\n",
    "                    p.imap(load_audio_task, file_list), total=len(file_list), disable=not self.verbose)\n",
    "            ]\n",
    "        # res_list = [load_audio_task(fn) for fn in file_list]\n",
    "        return res_list\n",
    "\n",
    "    # This function is the same as the parent's function, except that audio_background and audio_eval are loaded with \n",
    "    # the load_audio_files_from_list function, instead of the default __load_audio_files that takes directories as input\n",
    "    # It would be easier to override __load_audio_files directly, but this cannot be done since it's a private method.\n",
    "    def score(self, background_files, eval_files, store_embds=False):\n",
    "        audio_background = self.load_audio_files_from_list(background_files)\n",
    "        audio_eval = self.load_audio_files_from_list(eval_files)\n",
    "\n",
    "        embds_background = self.get_embeddings(audio_background)\n",
    "        embds_eval = self.get_embeddings(audio_eval)\n",
    "\n",
    "        if store_embds:\n",
    "            np.save(\"embds_background.npy\", embds_background)\n",
    "            np.save(\"embds_eval.npy\", embds_eval)\n",
    "\n",
    "        assert len(embds_background) != 0 and len(embds_eval) != 0\n",
    "        \n",
    "        mu_background, sigma_background = self.calculate_embd_statistics(embds_background)\n",
    "        mu_eval, sigma_eval = self.calculate_embd_statistics(embds_eval)\n",
    "\n",
    "        fad_score = self.calculate_frechet_distance(\n",
    "            mu_background, \n",
    "            sigma_background, \n",
    "            mu_eval, \n",
    "            sigma_eval\n",
    "        )\n",
    "\n",
    "        return fad_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/passch/.cache/torch/hub/harritaylor_torchvggish_master\n"
     ]
    }
   ],
   "source": [
    "frechet = CustomFAD(\n",
    "    use_pca=False, \n",
    "    use_activation=False,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Inception Score, we use a custom class that makes use of the `sklearn` MLP we trained in `src/notebooks/speech_classifier.ipynb` (see the implementation in `src/utils/inception_score.py` for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_score = InceptionScore(clf_path='../../exp/speech_classifier/speech_clf_pipeline_variants_aug.pickle', verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VariaNTS-based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the files for the VariaNTS dataset\n",
    "variants_path = \"../../data/VariaNTS/VariaNTS_words_16kHz_HP_synth_aug_flattened_fixed-length\"\n",
    "variants_files = [os.path.join(variants_path, fn) for fn in os.listdir(variants_path)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reference score for the FAD, we compute the FAD between real and artificial speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_speakers = [fn for fn in variants_files if int(fn.split('/')[-1].split('_')[0][1:]) <= 16]\n",
    "fake_speakers = [fn for fn in variants_files if int(fn.split('/')[-1].split('_')[0][1:]) > 16]\n",
    "assert len(real_speakers) == len(fake_speakers)\n",
    "\n",
    "fad_score = frechet.score(real_speakers, fake_speakers)\n",
    "print('Real vs. Fake:', round(fad_score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_score = inception_score(variants_files)\n",
    "print('IS Real+Fake:', round(inc_score, 4))\n",
    "\n",
    "inc_score = inception_score(real_speakers)\n",
    "print('IS Real:', round(inc_score, 4))\n",
    "\n",
    "inc_score = inception_score(fake_speakers)\n",
    "print('IS Fake:', round(inc_score, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SG-U (No augs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = \"../../exp/SG-U_v9_noaug/waveforms/1000/all\"\n",
    "eval_files = [os.path.join(eval_path, fn) for fn in os.listdir(eval_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29920/29920 [00:08<00:00, 3569.44it/s]\n",
      "100%|██████████| 880/880 [00:02<00:00, 338.91it/s]\n",
      "100%|██████████| 29920/29920 [02:47<00:00, 178.50it/s]\n",
      "100%|██████████| 880/880 [00:04<00:00, 181.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.352"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fad_score = frechet.score(variants_files, eval_files)\n",
    "round(fad_score, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 880/880 [00:19<00:00, 44.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0851"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc_score = inception_score(eval_files)\n",
    "round(inc_score, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SG-U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = \"../../exp/SG-U_v9/waveforms/230/all\"\n",
    "eval_files = [os.path.join(eval_path, fn) for fn in os.listdir(eval_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29920/29920 [00:09<00:00, 3096.30it/s]\n",
      "100%|██████████| 880/880 [00:00<00:00, 3516.14it/s]\n",
      "100%|██████████| 29920/29920 [02:47<00:00, 178.69it/s]\n",
      "100%|██████████| 880/880 [00:04<00:00, 181.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.3348"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fad_score = frechet.score(variants_files, eval_files)\n",
    "round(fad_score, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 880/880 [00:18<00:00, 47.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.0429"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc_score = inception_score(eval_files)\n",
    "round(inc_score, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SG-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = \"/home/passch/exp/ClassCond-PT-v3_h256_d36_T200_betaT0.02_L1000_cond/waveforms/180/all\"\n",
    "eval_files = [os.path.join(eval_path, fn) for fn in os.listdir(eval_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29920/29920 [00:07<00:00, 3853.32it/s]\n",
      "100%|██████████| 880/880 [00:02<00:00, 323.01it/s]\n",
      "100%|██████████| 29920/29920 [02:54<00:00, 171.13it/s]\n",
      "100%|██████████| 880/880 [00:04<00:00, 181.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.3266"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fad_score = frechet.score(variants_files, eval_files)\n",
    "round(fad_score, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 880/880 [00:12<00:00, 69.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.5953"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc_score = inception_score(eval_files)\n",
    "round(inc_score, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2S-Cv (Brain- & Class-conditional Finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = \"/home/passch/exp/BrainClassCond-FT-VariaNTS-v9_h256_d36_T200_betaT0.02_L1000_cond/waveforms/800\"\n",
    "\n",
    "eval_files_train = [os.path.join(eval_path, \"train\", fn) for fn in os.listdir(os.path.join(eval_path, \"train\"))]\n",
    "eval_files_val = [os.path.join(eval_path, \"val\", fn) for fn in os.listdir(os.path.join(eval_path, \"val\"))]\n",
    "eval_files = [*eval_files_train, *eval_files_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29920/29920 [00:09<00:00, 3026.37it/s]\n",
      "100%|██████████| 1008/1008 [00:03<00:00, 314.91it/s]\n",
      "100%|██████████| 29920/29920 [02:56<00:00, 169.17it/s]\n",
      "100%|██████████| 1008/1008 [00:05<00:00, 178.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.3273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29920/29920 [00:09<00:00, 3107.94it/s]\n",
      "100%|██████████| 848/848 [00:00<00:00, 3069.81it/s]\n",
      "100%|██████████| 29920/29920 [02:49<00:00, 176.52it/s]\n",
      "100%|██████████| 848/848 [00:04<00:00, 180.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.3276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29920/29920 [00:09<00:00, 3300.39it/s]\n",
      "100%|██████████| 160/160 [00:00<00:00, 3103.76it/s]\n",
      "100%|██████████| 29920/29920 [02:48<00:00, 177.25it/s]\n",
      "100%|██████████| 160/160 [00:00<00:00, 184.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.3257\n"
     ]
    }
   ],
   "source": [
    "fad_score = frechet.score(variants_files, eval_files)\n",
    "print(round(fad_score, 4))\n",
    "\n",
    "fad_score = frechet.score(variants_files, eval_files_train)\n",
    "print(round(fad_score, 4))\n",
    "\n",
    "fad_score = frechet.score(variants_files, eval_files_val)\n",
    "print(round(fad_score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1008/1008 [00:18<00:00, 54.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 848/848 [00:07<00:00, 114.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.3938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:01<00:00, 89.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9999\n"
     ]
    }
   ],
   "source": [
    "inc_score = inception_score(eval_files)\n",
    "print(round(inc_score, 4))\n",
    "\n",
    "inc_score = inception_score(eval_files_train)\n",
    "print(round(inc_score, 4))\n",
    "\n",
    "inc_score = inception_score(eval_files_val)\n",
    "print(round(inc_score, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainconditional Finetuning (VariaNTS speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = \"/home/passch/exp/BrainCond-FT-VariaNTS-v3_h256_d36_T200_betaT0.02_L1000_cond/waveforms/140\"\n",
    "\n",
    "eval_files_train = [os.path.join(eval_path, \"train\", fn) for fn in os.listdir(os.path.join(eval_path, \"train\"))]\n",
    "eval_files_val = [os.path.join(eval_path, \"val\", fn) for fn in os.listdir(os.path.join(eval_path, \"val\"))]\n",
    "eval_files = [*eval_files_train, *eval_files_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29920/29920 [00:09<00:00, 3092.47it/s]\n",
      "100%|██████████| 1008/1008 [00:02<00:00, 349.12it/s]\n",
      "100%|██████████| 29920/29920 [02:50<00:00, 175.65it/s]\n",
      "100%|██████████| 1008/1008 [00:06<00:00, 165.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29920/29920 [00:09<00:00, 3301.65it/s]\n",
      "100%|██████████| 848/848 [00:00<00:00, 3278.55it/s]\n",
      "100%|██████████| 29920/29920 [02:51<00:00, 174.95it/s]\n",
      "100%|██████████| 848/848 [00:04<00:00, 179.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.3334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29920/29920 [00:10<00:00, 2972.45it/s]\n",
      "100%|██████████| 160/160 [00:00<00:00, 2637.80it/s]\n",
      "100%|██████████| 29920/29920 [02:47<00:00, 178.71it/s]\n",
      "100%|██████████| 160/160 [00:00<00:00, 167.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.3332\n"
     ]
    }
   ],
   "source": [
    "fad_score = frechet.score(variants_files, eval_files)\n",
    "print(round(fad_score, 4))\n",
    "\n",
    "fad_score = frechet.score(variants_files, eval_files_train)\n",
    "print(round(fad_score, 4))\n",
    "\n",
    "fad_score = frechet.score(variants_files, eval_files_val)\n",
    "print(round(fad_score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1008/1008 [00:18<00:00, 55.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.4359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 848/848 [00:09<00:00, 93.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.9151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:01<00:00, 94.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.9639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inc_score = inception_score(eval_files)\n",
    "print(round(inc_score, 4))\n",
    "\n",
    "inc_score = inception_score(eval_files_train)\n",
    "print(round(inc_score, 4))\n",
    "\n",
    "inc_score = inception_score(eval_files_val)\n",
    "print(round(inc_score, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction-based Model B2S-Ur\n",
    "\n",
    "There is only one model trained to reconstruct the actual speaker voice recorded during the reading task, B2S-Ur. Because of this, we cannot compare the objective metrics with those of the other models above. Nonetheless, the below code computes the FAD for completeness, in case this becomes relevant at some point. Since we did not train a speech classifier for the speaker's data, there is no code to compute the Inception Score, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files for the speech dataset recorded during reading of the Harry Potter chapter\n",
    "hp_audio_path = \"../../data/HP1_ECoG_conditional/sub-002_fixed-length\"\n",
    "hp_files = [os.path.join(hp_audio_path, fn) for fn in os.listdir(hp_audio_path) if fn.endswith('.wav')]\n",
    "\n",
    "# with open('/home/passch/data/datasplits/HP1_ECoG_conditional/sub-002/train.csv', 'r') as f:\n",
    "#     hp_train_files = f.read().split(',')\n",
    "# with open('/home/passch/data/datasplits/HP1_ECoG_conditional/sub-002/val.csv', 'r') as f:\n",
    "#     hp_val_files = f.read().split(',')\n",
    "# hp_train_files = [os.path.join(hp_audio_path, fn) for fn in hp_train_files]\n",
    "# hp_val_files = [os.path.join(hp_audio_path, fn) for fn in hp_val_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:25<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5854788612242084 0.06481413255675972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For HP reference score, randomly separate the data 50 times and compute mean\n",
    "# and variance of the FADs of all of them \n",
    "hp_fad_scores = []\n",
    "for i in tqdm(range(50)):\n",
    "    half = len(hp_files) // 2\n",
    "    np.random.shuffle(hp_files)\n",
    "    fad_score = frechet.score(hp_files[:half], hp_files[half:])\n",
    "    hp_fad_scores.append(fad_score)\n",
    "print(np.mean(hp_fad_scores), np.std(hp_fad_scores))\n",
    "hp_files = sorted(hp_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model generated files for train and val set\n",
    "eval_path = \"../../exp/B2S-UR_v5/waveforms/70\"\n",
    "\n",
    "eval_files_train = [os.path.join(eval_path, \"train\", fn) for fn in os.listdir(os.path.join(eval_path, \"train\"))]\n",
    "eval_files_val = [os.path.join(eval_path, \"val\", fn) for fn in os.listdir(os.path.join(eval_path, \"val\"))]\n",
    "eval_files = [*eval_files_train, *eval_files_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0585\n",
      "6.0584\n",
      "6.059\n"
     ]
    }
   ],
   "source": [
    "fad_score = frechet.score(hp_files, eval_files)\n",
    "print(round(fad_score, 4))\n",
    "\n",
    "fad_score = frechet.score(hp_files, eval_files_train)\n",
    "print(round(fad_score, 4))\n",
    "\n",
    "fad_score = frechet.score(hp_files, eval_files_val)\n",
    "print(round(fad_score, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffwave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a17d9c9cbbc962de36b255476254331786dd384e23a24706cf6aa287aa1626b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
